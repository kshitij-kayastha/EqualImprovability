{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import torch.optim as optim\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from model import LR, NN\n",
    "from data import FairnessDataset, SyntheticDataset, GermanDataset\n",
    "from ei_model_dev import FairBatch, Covariance\n",
    "from ei_effort import Optimal_Effort, PGD_Effort\n",
    "from ei_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SyntheticDataset(seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_res(d, lamb, alpha, acc, ei, model, model_adv):\n",
    "    d['lambda'].append(lamb)\n",
    "    d['alpha'].append(alpha)\n",
    "    d['accuracy'].append(acc)\n",
    "    d['loss'].append(1-acc)\n",
    "    d['ei_disparity'].append(ei)\n",
    "    d['model'].append(model)\n",
    "    d['model_adv'].append(model_adv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_runner(ei_model, dataset, hp, results):\n",
    "    tau = 0.5\n",
    "    pga_term = hp['pga_term']\n",
    "    \n",
    "    if hp['optimal_effort']:\n",
    "        effort = Optimal_Effort(hp['delta'])\n",
    "    else:\n",
    "        effort = PGD_Effort(hp['delta'])\n",
    "    \n",
    "    train_tensors, val_tensors, test_tensors = dataset.tensor(z_blind=hp['z_blind'])\n",
    "    train_dataset = FairnessDataset(*train_tensors, dataset.imp_feats)\n",
    "    val_dataset = FairnessDataset(*val_tensors, dataset.imp_feats)\n",
    "    test_dataset = FairnessDataset(*test_tensors, dataset.imp_feats)\n",
    "    \n",
    "    model = LR(num_features=train_dataset.X.shape[1])\n",
    "    ei_m = ei_model(model, effort, pga_term, tau)\n",
    "    \n",
    "    ei_m.train(\n",
    "        train_dataset, \n",
    "        lamb=hp['lambda'],\n",
    "        lr=hp['learning_rate'],\n",
    "        alpha=0.,\n",
    "        batch_size=1024\n",
    "        )\n",
    "    \n",
    "    Y_hat, Y_hat_max = ei_m.predict(test_dataset)\n",
    "    test_acc, test_ei = model_performance(test_dataset.Y.detach().numpy(), test_dataset.Z.detach().numpy(), Y_hat, Y_hat_max, tau)\n",
    "    append_res(results, hp['lambda'], 0., test_acc, test_ei, ei_m.model, ei_m.model_adv)\n",
    "    ei_models = [ei_m.model]\n",
    "    \n",
    "    for alpha in hp['alpha']:\n",
    "        Y_hat_r, Y_hat_max_r = ei_m.predict_r(test_dataset, alpha)\n",
    "        test_acc_r, test_ei_r = model_performance(test_dataset.Y.detach().numpy(), test_dataset.Z.detach().numpy(), Y_hat_r, Y_hat_max_r, tau)\n",
    "        append_res(results, hp['lambda'], alpha, test_acc_r, test_ei_r, ei_m.model, ei_m.model_adv)\n",
    "        ei_models.append(ei_m.model_adv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_tradeoff(ei_model, dataset, hyper_params):\n",
    "    hp = hyper_params.copy()\n",
    "    results = pd.DataFrame()\n",
    "    \n",
    "    results = {'lambda': [], 'alpha': [], 'accuracy': [], 'loss': [], 'ei_disparity': [], 'model': [], 'model_adv': []}\n",
    "    for delta in hyper_params['delta']:\n",
    "        for lamb in hyper_params['lambda']:\n",
    "            hp['lambda'] = lamb\n",
    "            hp['delta'] = delta\n",
    "            \n",
    "            model_runner(ei_model, dataset, hp, results)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "# Hyperparameters\n",
    "hyper_params = {}\n",
    "hyper_params['delta'] = [0.5]\n",
    "hyper_params['alpha'] = [0.1, 0.2005, 0.2075, 0.2081, 0.5]\n",
    "hyper_params['learning_rate'] = 0.01\n",
    "hyper_params['pga_term'] = 20\n",
    "hyper_params['z_blind'] = False\n",
    "hyper_params['optimal_effort'] = True # True only for Synthetic Data\n",
    "\n",
    "ei_model = FairBatch\n",
    "# hyper_params['lambda'] = np.linspace(0., 0.25, 10) # FairBatch lambdas\n",
    "# hyper_params['lambda'] = [0.88] # lambda value that minimizes ei for FairBatch\n",
    "hyper_params['lambda'] = [0.] # lambda value that minimizes ei for FairBatch\n",
    "\n",
    "# Run tradeoffs\n",
    "results = run_tradeoff(ei_model, dataset, hyper_params)\n",
    "# results['alpha'] = results['alpha'].round(2)\n",
    "# results_r['alpha'] = results_r['alpha'].round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = {\n",
    "    'lambda': results['lambda'],\n",
    "    'alpha': results['alpha'],\n",
    "    'accuracy': results['accuracy'],\n",
    "    'loss': results['loss'],\n",
    "    'ei_disparity': results['ei_disparity']\n",
    "}\n",
    "res = pd.DataFrame(res)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = None\n",
    "for i in range(6):\n",
    "    print(f\"lambda: {results['lambda'][i]}, alpha: {results['alpha'][i]}\")\n",
    "    model, model_adv = results['model'][i], results['model_adv'][i]\n",
    "    for module in model.layers:\n",
    "        if hasattr(module, 'weight'):\n",
    "            weights_0 = module.weight.data\n",
    "        print(weights_0)\n",
    "        if hasattr(module, 'bias'):\n",
    "            bias_0 = module.bias.data\n",
    "        theta_0 = torch.cat((weights_0[0], bias_0), 0)\n",
    "        print(bias_0)\n",
    "    # print(f'Model: {theta_0}')\n",
    "        \n",
    "    # for module in model_adv.layers:\n",
    "    #     if hasattr(module, 'weight'):\n",
    "    #         weights = module.weight.data\n",
    "    #     if hasattr(module, 'bias'):\n",
    "    #         bias = module.bias.data\n",
    "    #     theta = torch.cat((weights[0], bias), 0)\n",
    "    # print(f'Model Adv: {theta}')\n",
    "    # print(f'alphas: {theta_0-theta}')\n",
    "    \n",
    "    # print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tensors, val_tensors, test_tensors = dataset.tensor(z_blind=False)\n",
    "train_dataset = FairnessDataset(*train_tensors, dataset.imp_feats)\n",
    "val_dataset = FairnessDataset(*val_tensors, dataset.imp_feats)\n",
    "test_dataset = FairnessDataset(*test_tensors, dataset.imp_feats)\n",
    "\n",
    "model_params = torch.load('../ei_model.pkl')\n",
    "model = LR(train_dataset.X.shape[1])\n",
    "model.load_state_dict(model_params)\n",
    "\n",
    "for module in model.layers:\n",
    "    if hasattr(module, 'weight'):\n",
    "        weights = module.weight.data\n",
    "    if hasattr(module, 'bias'):\n",
    "        bias = module.bias.data\n",
    "\n",
    "theta_0 = torch.cat((weights[0], bias), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.2\n",
    "grid = generate_grid(theta_0.numpy(), alpha, n=5, ord=np.inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(model, grid, dataset, tau=0.5):\n",
    "    Y_hat = model(dataset.X).reshape(-1).detach().numpy()\n",
    "    X_hat_max = Optimal_Effort(tau)(model, dataset, dataset.X)\n",
    "\n",
    "    accuracies, ei_disparities = [], []\n",
    "    \n",
    "    for theta in tqdm(grid[:1]):\n",
    "        weights, bias = theta[:-1].reshape(1, -1), theta[[-1]]\n",
    "        # print(weights, bias)\n",
    "        model_adv = deepcopy(model)\n",
    "        for module in model_adv.layers:\n",
    "            if hasattr(module, 'weight'):\n",
    "                module.weight.data = weights.float()\n",
    "            if hasattr(module, 'bias'):\n",
    "                module.bias.data = bias.float()\n",
    "                \n",
    "        Y_hat_max = model_adv(X_hat_max)\n",
    "        \n",
    "        Y_pred = (Y_hat>tau)*1\n",
    "        Y_pred_max = (Y_hat_max>tau)*1\n",
    "        \n",
    "        n_eyz = {}\n",
    "        for y_max in [0,1]: \n",
    "            for y in [0,1]:\n",
    "                for z in [0,1]:\n",
    "                    n_eyz[(y_max,y,z)] = np.sum((Y_pred_max==y_max)*(Y_pred==y)*(dataset.Z==z))\n",
    "                    \n",
    "        print(n_eyz)\n",
    "        z_set = list(set([z for _,_, z in n_eyz.keys()]))\n",
    "        sum([n_eyz[(1,0,z)]+n_eyz[(0,0,z)] for z in z_set])\n",
    "    \n",
    "        \n",
    "    #     acc, ei_d = model_performance(dataset.Y.detach().numpy(), dataset.Z.detach().numpy(), Y_hat, Y_hat_max.detach().numpy(), 0.5)\n",
    "    #     accuracies.append(acc)\n",
    "    #     ei_disparities.append(ei_d)\n",
    "        \n",
    "    # max_i = np.argmax(ei_disparities)\n",
    "    \n",
    "    # return accuracies[max_i], ei_disparities[max_i], grid[max_i], ei_disparities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 625/625 [00:30<00:00, 20.23it/s]\n"
     ]
    }
   ],
   "source": [
    "# acc, ei_d, theta, ei_d_max = grid_search(model, grid, test_dataset)\n",
    "grid_search(model, grid, test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0218,  0.3095, -0.3942,  0.1345], dtype=torch.float64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_eyz = {}\n",
    "for y_max in [0,1]: \n",
    "    for y in [0,1]:\n",
    "        for z in [0,1]:\n",
    "            n_eyz[(y_max,y,z)] = np.sum((Y_pred_max==y_max)*(Y_pred==y)*(Z==z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.2218,  0.5095, -0.1942,  0.3345])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('EI (train alpha = 0)')\n",
    "results[['alpha', 'lambda', 'loss', 'ei_disparity']]#.sort_values(['lambda', 'alpha']).set_index(['lambda', 'alpha'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('REI (train alpha = a)')\n",
    "results_r[['alpha', 'lambda', 'loss', 'ei_disparity']]##.sort_values(['lambda', 'alpha']).set_index(['lambda', 'alpha'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment this to save the results\n",
    "# results.to_pickle(f'robust_ei_optimal_lambda_tradeoff_{ei_proxy.lower()}_synthetic_5crossval.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the pareto frontier\n",
    "results_pareto = pd.DataFrame()\n",
    "for alpha in results['alpha'].unique():\n",
    "    test_results_alpha = results[results['alpha'] == alpha]\n",
    "    mask = pareto_frontier(test_results_alpha['loss_mean'], test_results_alpha['ei_disparity_mean'])\n",
    "    results_alpha_pareto = test_results_alpha.iloc[mask]\n",
    "    results_pareto = pd.concat((results_pareto, results_alpha_pareto.sort_values('ei_disparity_mean')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(results, x='ei_disparity_mean', y='loss_mean', color='alpha', hover_data='lambda', markers=True)\n",
    "fig.add_annotation(dict(font=dict(color='black',size=10),\n",
    "                                        x=0.9,\n",
    "                                        y=0.99,\n",
    "                                        showarrow=False,\n",
    "                                        text='dataset=synthetic',\n",
    "                                        textangle=0,\n",
    "                                        xanchor='left',\n",
    "                                        xref=\"paper\",\n",
    "                                        yref=\"paper\"))\n",
    "fig.update_layout(title=dict(text='Fairness vs Loss Tradeoff', x=0.5))\n",
    "fig.update_traces(marker=dict(size=3))\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
