{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from model import LR\n",
    "from data import FairnessDataset, SyntheticDataset, GermanDataset, IncomeDataset\n",
    "from ei_effort import Optimal_Effort, PGD_Effort\n",
    "from ei_utils import *\n",
    "from ei_model_test import EIModel, fair_batch_proxy, covariance_proxy\n",
    "\n",
    "# warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = SyntheticDataset(seed=1)\n",
    "# dataset = GermanDataset(seed=1)\n",
    "dataset = IncomeDataset(seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training [alpha=0.00; lambda=0.00; delta=0.50]: 100%|\u001b[38;2;0;145;255m██████████\u001b[0m| 100/100 [03:38<00:00,  2.19s/epochs]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 4.7478e-01,  1.5598e-01,  7.7882e-01, -3.4081e-01, -2.6925e-01,\n",
      "         3.3248e-04, -1.9050e-02,  3.6647e-03, -1.0576e+00, -5.0976e-01,\n",
      "        -1.0065e+00, -4.3150e-01, -5.9548e-01, -4.9879e-01, -8.0174e-01,\n",
      "        -8.1878e-01,  1.0859e+00,  7.9898e-01,  3.5037e-01,  8.7859e-01,\n",
      "         1.0070e+00,  4.7153e-01,  2.9697e-01,  5.0400e-01,  1.2338e+00,\n",
      "         1.1365e+00,  1.3547e-01,  9.8474e-01,  4.4192e-01,  6.1603e-01,\n",
      "         5.8151e-01,  4.6593e-02,  5.2171e-02,  8.4468e-01,  3.6631e-01,\n",
      "         1.3328e+00, -1.8848e-01,  5.2787e-01,  1.1348e-01,  6.5862e-01,\n",
      "         1.2283e-01,  2.2122e-01,  5.3878e-01,  4.9754e-02,  8.0645e-01,\n",
      "        -1.8561e-02, -6.8724e-02, -3.5685e-01,  5.7145e-01,  3.1061e-01,\n",
      "         7.0224e-01,  4.6375e-01,  8.3738e-01,  2.9364e-01,  3.9992e-01,\n",
      "        -4.1479e-02,  1.3246e+00,  1.0971e+00,  7.6897e-02,  4.5306e-01,\n",
      "         8.2388e-01,  3.9011e-01,  5.8332e-01,  1.7734e-01,  3.3326e-01,\n",
      "         2.3058e-01,  1.0258e+00,  9.4341e-01,  4.3975e-01,  1.3526e-01,\n",
      "         3.3987e-01, -1.0310e-01,  2.9976e-02,  3.0960e-01,  5.2640e-01,\n",
      "         9.4646e-01,  5.8910e-01,  1.0987e+00,  1.9030e+00,  5.9736e-01,\n",
      "         5.2552e-01,  4.3092e-01,  3.0683e-01,  7.0545e-01,  8.1204e-01,\n",
      "         7.9735e-01,  6.0683e-01,  3.3249e-01,  8.2650e-01,  1.0940e+00,\n",
      "         8.2016e-01,  5.3216e-01,  7.7255e-02,  1.1623e+00,  3.8982e-01,\n",
      "         4.4544e-01,  1.1536e+00,  6.1745e-01,  1.3466e+00,  3.6462e-01,\n",
      "         7.6227e-01,  7.8976e-02,  3.9911e-01,  9.7697e-01,  1.6176e-01,\n",
      "         1.5754e+00,  4.1011e-01,  1.9446e-01,  1.0893e-01,  4.0285e-01,\n",
      "         3.7389e-01,  5.6809e-03,  3.7857e-01, -4.3145e-03,  6.1629e-01,\n",
      "         6.5118e-02, -1.5348e-02,  2.8647e-01, -1.4744e-02,  2.7645e-01,\n",
      "         4.0958e-01,  3.0078e-01,  3.1793e-02,  4.2956e-01,  7.4137e-01,\n",
      "         3.4456e-01,  1.6590e-01,  1.5767e-01, -2.2401e-02,  1.5331e-01,\n",
      "         2.6313e-01, -5.3031e-01,  3.4139e-01, -3.5490e-01,  2.5198e-01,\n",
      "         9.6977e-02, -1.1591e-01, -4.4816e-02,  7.9216e-03, -1.6422e-01,\n",
      "         2.4852e-01,  1.5331e-01,  1.2598e-01,  4.5558e-01, -3.7253e-01,\n",
      "         1.5457e-01, -6.0446e-01, -1.7470e-01, -3.1116e-01,  1.2704e+00,\n",
      "         7.2089e-02,  3.2372e-01,  2.4457e-01,  1.6869e-01, -1.5941e-01,\n",
      "        -1.2429e+00,  1.8093e-01,  4.2175e-01, -1.6898e-01, -6.2838e-01,\n",
      "        -1.9803e-01, -1.4170e-01, -5.1117e-02, -2.9377e-01, -1.7144e+00,\n",
      "        -2.8866e-02,  3.4299e-01,  3.2242e-01,  1.7653e-01, -2.5734e-01,\n",
      "         1.8682e-01,  1.8950e-01, -8.4839e-02,  5.1240e-01, -1.7404e-01,\n",
      "         4.1388e-01, -6.1991e-02, -4.5572e-01, -1.4181e-01, -1.3889e-01,\n",
      "        -3.8080e-01,  9.5588e-02, -5.5814e-02,  1.2213e-01,  1.0107e-01,\n",
      "        -5.5891e-03,  4.4798e-01,  2.9023e-01,  6.8920e-01,  2.3586e-01,\n",
      "        -1.7513e-01,  3.3373e-01,  1.8179e-01,  1.6401e-01, -3.3299e-01,\n",
      "         4.7914e-01,  1.4829e-01,  8.6467e-01,  1.3283e-01,  1.1617e-01,\n",
      "         1.0647e+00,  1.3383e+00,  2.4793e-01,  5.7508e-01, -1.0406e-01,\n",
      "         9.8743e-02,  8.3544e-01,  9.8797e-01,  2.9916e-01,  4.8721e-02,\n",
      "         7.0479e-01,  5.3952e-01,  9.4102e-02,  4.2716e-01,  1.6806e+00,\n",
      "         1.7850e-01,  9.2665e-01, -1.0467e-01, -6.1750e-02,  2.4049e-01,\n",
      "         9.4384e-01,  9.3518e-02,  7.6811e-01,  9.9332e-01,  4.4590e-01,\n",
      "         4.3270e-01, -6.8434e-01, -1.1607e-01, -4.9368e-01, -7.9288e-02,\n",
      "         3.8572e-01, -5.1322e-01, -3.0266e-01, -3.3901e-01, -2.9332e-02,\n",
      "         7.7809e-02,  1.3166e-01,  1.2049e-01, -9.3512e-01, -2.1949e+00,\n",
      "        -1.1300e+00, -5.2360e-01, -2.1301e-01, -1.0890e-01, -6.6558e-01,\n",
      "        -7.9237e-01, -8.2923e-01, -3.6088e-01, -1.7378e-02, -4.0952e-01,\n",
      "        -5.4524e-01, -3.7999e-01,  3.7514e-01,  5.3133e-01,  3.5269e-01,\n",
      "        -4.7546e-03,  2.7064e-01, -1.6497e-02,  2.4553e-01,  8.2928e-01,\n",
      "         7.6152e-01,  1.2513e-02,  9.7852e-01, -1.9793e-02,  1.8556e-01,\n",
      "        -9.6368e-01, -4.6168e-01, -3.1311e-01, -6.2821e-02, -2.8893e-01,\n",
      "        -8.6806e-01, -8.1862e-01, -1.6551e+00, -1.5400e+00, -7.5914e-01,\n",
      "        -1.1425e+00, -1.3942e+00, -6.2219e-01, -1.0193e+00, -9.8424e-01,\n",
      "        -6.0271e-01, -8.8610e-02, -1.5486e-01, -1.2209e-01, -1.2346e+00,\n",
      "        -1.3881e+00, -3.5366e-01, -1.0340e+00, -2.4696e-01, -1.2035e-01,\n",
      "        -2.7271e-01, -3.1401e-01, -8.8888e-01, -3.6758e-01, -1.8214e-01,\n",
      "        -8.0865e-01,  8.6012e-03,  4.5930e-02, -4.1677e-01, -6.3927e-01,\n",
      "        -1.2000e+00, -5.6056e-01, -1.0091e-01, -2.7858e-01, -4.9838e-01,\n",
      "        -1.5334e+00, -5.0420e-01, -6.4532e-01, -2.4233e-01, -5.6264e-01,\n",
      "        -2.3288e-01,  6.2146e-01, -1.7754e+00, -3.6246e-01, -2.0482e-01,\n",
      "        -8.3395e-01,  2.7345e-01,  2.7502e-01,  5.3828e-01, -5.4222e-01,\n",
      "         4.8414e-01,  3.4144e-01, -2.1930e-02,  4.6831e-01,  2.3385e-01,\n",
      "        -4.4571e-01, -4.2895e-01,  1.5458e-01,  2.5245e-01, -2.0646e-01,\n",
      "        -1.9108e-01,  2.8771e-02, -5.6212e-01, -7.2640e-01, -4.9712e-01,\n",
      "        -2.6316e-02,  2.2583e-01, -7.2815e-01,  3.6099e-01, -2.3644e-01,\n",
      "        -1.8575e-01, -6.5831e-01, -3.9891e-01, -6.7190e-01, -8.6787e-01,\n",
      "        -5.2340e-01, -8.9481e-01,  2.9881e-01, -8.3931e-02, -5.0655e-01,\n",
      "        -2.9592e-01, -1.4732e+00, -2.2732e-01, -4.0032e-01, -9.9625e-02,\n",
      "        -7.3927e-01,  2.9427e-01, -2.4996e-01,  2.4923e-01,  1.8240e-01,\n",
      "         2.5810e-01,  1.0183e-01,  2.3936e-01, -1.0837e+00, -3.3907e-01,\n",
      "         6.4116e-01,  2.6598e-02, -3.1650e-01, -6.2195e-01, -7.4556e-01,\n",
      "        -8.6404e-01, -8.2516e-02, -2.5176e-01, -7.9758e-01, -1.9079e-01,\n",
      "        -8.4802e-02,  1.6179e-01, -3.4839e-01, -4.6675e-02, -2.8283e-01,\n",
      "        -7.1393e-01, -2.0028e+00, -1.2980e-01,  1.4798e-01, -2.8943e-02,\n",
      "         6.4123e-01,  3.4037e-02, -1.4653e-01, -5.5320e-02, -1.1711e-01,\n",
      "         2.5682e-01, -3.1605e-01,  3.3863e-01, -6.2662e-02,  3.7375e-01,\n",
      "         1.7503e-01, -2.5422e-02, -6.6908e-01, -1.3405e-02,  2.9993e-02,\n",
      "        -1.3464e-01, -2.1218e-01,  1.0566e-02,  2.7583e-01, -1.5584e-01,\n",
      "        -1.0604e-01,  3.6813e-01,  3.2516e-01,  2.0193e-02,  8.8812e-02,\n",
      "        -7.3429e-03,  5.0286e-06,  1.6271e-01,  3.2738e-02,  2.0552e-01,\n",
      "        -6.3177e-02, -2.2424e-02, -2.9509e-02,  1.0006e-01, -1.2086e-01,\n",
      "         3.1049e-01, -9.7873e-02,  9.8912e-02,  1.0769e-01, -1.6411e-01,\n",
      "        -6.3691e-02,  3.5126e-01,  2.4834e-01, -1.3895e-01, -4.2790e-01,\n",
      "         1.3337e-01,  2.4053e-01, -2.0296e-01, -3.1322e-01,  2.6896e-01,\n",
      "         1.9704e-01, -1.0439e-02,  3.0600e-02, -1.2311e-01, -4.4704e-02,\n",
      "         8.7721e-02,  4.7987e-01,  4.2701e-01, -1.2160e-03, -1.8858e-01,\n",
      "        -3.5321e-02,  5.0280e-02, -1.2197e-01, -3.4626e-01,  3.1826e-01,\n",
      "        -5.3354e-01,  1.7739e-02,  4.6165e-02, -9.4236e-01, -8.6146e-01,\n",
      "        -5.5640e-01, -1.7774e-02, -5.8276e-01, -4.3264e-04, -3.0048e-01,\n",
      "        -8.3933e-03,  1.5430e-02, -2.5594e-02, -2.8958e-01, -1.7218e-01,\n",
      "        -2.2639e-01, -1.0420e-02, -1.8737e-01,  6.2574e-02, -1.8779e-01,\n",
      "        -4.0899e-01, -1.3434e-01, -3.8565e-01, -1.1123e-01, -6.6234e-01,\n",
      "        -1.4233e-01, -9.0149e-01,  7.2352e-02, -2.8369e-01, -2.1285e-01,\n",
      "        -2.9459e-01,  5.6399e-02, -3.1996e-01, -1.5218e-01, -1.2904e-01,\n",
      "        -6.2008e-02, -4.8334e-01,  4.7889e-01,  3.5333e-01,  2.8527e-01,\n",
      "         3.5167e-01, -2.6410e-01, -2.0823e-01, -1.5265e-01,  4.0313e-02,\n",
      "        -7.2932e-02, -1.1462e-01, -4.3133e-01, -4.1674e-01, -9.5172e-01,\n",
      "        -3.8109e-01,  9.5158e-02,  2.1563e-02, -6.8628e-02, -5.6225e-03,\n",
      "        -1.7282e-01,  3.4753e-03, -3.6137e-01, -7.1894e-01, -8.2954e-02,\n",
      "         7.2413e-01,  1.7635e-01, -5.2878e-02, -9.2672e-02, -8.0132e-01,\n",
      "        -6.3983e-01, -5.1174e-01, -9.8439e-01, -1.3317e+00, -6.4557e-01,\n",
      "         2.7304e-01,  1.0022e-01,  1.6233e-01,  1.3616e-02,  5.8279e-02,\n",
      "        -7.1034e-01, -4.5616e-01, -9.4987e-02, -1.1322e-01,  1.0515e-01,\n",
      "        -1.7880e-01,  1.3852e-01, -9.4282e-01, -8.3202e-01, -1.0257e+00,\n",
      "        -3.0760e-01, -1.5025e+00, -1.1590e+00,  1.0201e-01, -3.8925e-01,\n",
      "        -1.0785e-01, -1.3251e-01, -1.0997e-01, -1.7187e-01,  8.9758e-02,\n",
      "         2.7309e-01,  1.2050e-01,  6.5985e-02,  4.0209e-02,  1.4791e-02,\n",
      "         3.0519e-01,  1.1201e-01,  3.0219e-01,  2.1075e-01,  8.0235e-02,\n",
      "        -3.7872e-03,  2.4612e-01, -1.4759e-02,  1.7002e-01, -1.2134e-01,\n",
      "         2.7833e-01,  4.5300e-02,  1.4530e-01, -9.0319e-02,  9.1856e-02,\n",
      "        -1.3333e-01,  2.0758e-01,  5.0646e-02,  8.8859e-02,  9.8409e-02,\n",
      "         1.0550e-01, -8.8744e-04, -9.4518e-02,  1.2552e-01,  1.6913e-01,\n",
      "         6.1731e-02, -4.5147e-02,  1.4233e-01,  2.2139e-01,  1.3085e-01,\n",
      "         2.1196e-01, -6.6499e-02,  1.5822e-01,  2.1846e-01,  1.2385e-01,\n",
      "        -3.9989e-02,  2.6596e-01,  4.8583e-02,  8.5440e-02,  1.5569e-01,\n",
      "         1.8608e-01,  1.7804e-01,  2.5084e-01,  6.2431e-02,  2.9081e-01,\n",
      "        -1.1432e-01,  6.8616e-02, -1.0208e-01, -1.7451e-01, -7.0646e-02,\n",
      "        -5.7100e-02, -9.0560e-02, -8.1375e-02,  4.6355e-02, -2.0468e-01,\n",
      "        -7.2611e-02,  1.7588e-01,  5.2979e-02,  2.8991e-01, -4.8359e-02,\n",
      "        -1.7657e-01, -1.5989e-01, -4.2517e-03,  1.5210e-01,  8.1179e-02,\n",
      "         2.5752e-02,  1.9099e-01,  3.1780e-01,  3.9290e-02, -9.8764e-02,\n",
      "         1.4960e-01,  6.0747e-02, -1.8357e-02,  7.4587e-03,  4.7040e-02,\n",
      "         2.0399e-01,  1.5549e-02,  1.0432e-01, -1.2789e-01, -2.4316e-01,\n",
      "         9.0816e-02,  3.4591e-01,  5.1347e-02, -1.6421e-02, -1.4217e-01,\n",
      "         8.6762e-02,  1.1817e-01, -2.6579e-01,  6.2024e-02,  3.3552e-01,\n",
      "        -1.9353e-02, -3.5583e-02, -1.1097e-02, -1.7478e-02, -1.5323e-01,\n",
      "         3.1912e-02,  9.5068e-02, -3.9143e-02, -3.7181e-01, -4.8773e-01,\n",
      "        -2.1686e-02, -4.8320e-01, -3.7644e-01, -1.1813e-01,  4.8637e-02,\n",
      "        -4.0996e-01, -2.7534e-01, -2.5171e-02, -1.0090e-01, -2.1779e-01,\n",
      "        -1.6199e-01, -6.5090e-01, -9.4059e-02, -1.0939e-01,  1.4779e-02,\n",
      "        -3.6478e-01, -5.8297e-02, -1.0165e-01,  4.0632e-02, -2.5020e-01,\n",
      "        -2.8104e-01, -4.9803e-01, -1.7423e-01,  1.5907e-01, -2.8365e-02,\n",
      "        -4.6810e-02, -2.0599e-01, -3.0155e-01, -8.7696e-02, -1.2279e-01,\n",
      "        -1.3390e-03, -3.7054e-01, -1.2032e-01,  6.8314e-02, -6.8822e-02,\n",
      "         6.3322e-02, -3.1047e-02,  1.3875e-01, -3.4193e-01,  1.2136e-01,\n",
      "        -1.8634e-02, -4.3383e-01, -4.5491e-01, -2.4812e-01, -8.5244e-02,\n",
      "         9.3025e-02,  1.1765e-02,  1.4414e-01,  4.4493e-03, -3.9775e-01,\n",
      "        -1.3729e-02,  1.1705e-01,  3.7139e-03, -2.4990e-03,  1.2001e-01,\n",
      "        -2.6415e-12, -1.9641e-04,  1.0233e-01,  1.8624e-01,  4.8093e-07,\n",
      "        -7.9325e-04, -6.1990e-02, -4.8316e-02,  2.6576e-02, -1.4731e-02,\n",
      "        -1.8574e-01,  1.4318e-01, -5.2277e-02, -5.8335e-02, -4.7005e-01,\n",
      "         1.0457e-01, -2.5008e-03, -1.5746e-02,  5.5526e-02, -2.0906e-01,\n",
      "        -1.3591e-01,  2.7813e-02, -2.6767e-03, -3.2137e-01, -1.5239e-03,\n",
      "         4.0666e-02, -7.4630e-02,  1.9901e-02, -8.9301e-02,  1.1897e-01,\n",
      "        -5.5898e-02, -2.7179e-02,  1.4683e-01, -3.2356e-01, -4.5080e-02,\n",
      "         9.5928e-02,  1.3949e-02, -5.0139e-02, -5.6290e-02, -7.4740e-02,\n",
      "         7.9477e-02,  8.1368e-02,  1.9060e-02,  5.2335e-03,  4.6481e-02,\n",
      "        -2.7635e-03, -5.0927e-02,  2.3796e-02,  2.2956e-02,  1.6463e-01,\n",
      "         1.1368e-01,  7.1926e-02, -1.1085e-01,  5.8333e-03,  1.9858e-07,\n",
      "         1.2075e-02, -2.7961e-01,  1.5419e-01, -5.1914e-02,  6.4191e-01,\n",
      "         4.2923e-01, -5.1836e-01, -4.4029e-01, -8.1448e-01, -2.2855e-01,\n",
      "        -4.2052e-01, -5.8499e-01, -5.1007e-01, -4.5641e-01, -4.7430e-01,\n",
      "         9.8543e-02,  2.5123e-03,  2.6198e-01, -1.6398e-02, -7.3304e-02,\n",
      "        -1.7312e-06, -2.0774e-09, -2.2480e-01, -5.5484e-01, -6.4183e-01,\n",
      "        -4.7920e-02, -5.7529e-01,  2.7853e-02, -5.7782e-01, -4.5889e-01,\n",
      "        -2.4190e-01, -4.6560e-01, -2.1937e+00])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "train_tensors, val_tensors, test_tensors = dataset.tensor(z_blind=False)\n",
    "train_dataset = FairnessDataset(*train_tensors, dataset.imp_feats)\n",
    "val_dataset = FairnessDataset(*val_tensors, dataset.imp_feats)\n",
    "test_dataset = FairnessDataset(*test_tensors, dataset.imp_feats)\n",
    "\n",
    "model = LR(train_dataset.X.shape[1])\n",
    "\n",
    "tau = 0.5\n",
    "delta = 0.5\n",
    "lamb = 1.\n",
    "alpha = 0.\n",
    "effort = Optimal_Effort(delta)\n",
    "proxy = covariance_proxy\n",
    "lr = 1e-2\n",
    "n_epochs = 100\n",
    "\n",
    "# EI Model\n",
    "ei_model = EIModel(deepcopy(model), proxy, effort, tau)\n",
    "ei_model.train(train_dataset, lamb, alpha, lr, n_epochs)\n",
    "for module in ei_model.model.layers:\n",
    "    if hasattr(module, 'weight'):\n",
    "        weights_0 = module.weight.data\n",
    "    if hasattr(module, 'bias'):\n",
    "        bias_0 = module.bias.data\n",
    "theta_0 = torch.cat((weights_0[0], bias_0), 0)\n",
    "print(theta_0)\n",
    "\n",
    "# # REI Model\n",
    "# rei_model = EIModel(deepcopy(model), proxy, effort, tau)\n",
    "# rei_model.train(train_dataset, lamb, alpha, n_epochs=200, abstol=1e-7)\n",
    "# for module in rei_model.model.layers:\n",
    "#     if hasattr(module, 'weight'):\n",
    "#         weights_r_0 = module.weight.data\n",
    "#     if hasattr(module, 'bias'):\n",
    "#         bias_r_0 = module.bias.data\n",
    "# theta_0_r = torch.cat((weights_r_0[0], bias_r_0), 0)\n",
    "\n",
    "# print(theta_0_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Parameters\n",
      "------------------------------------------------\n",
      "Dataset                   |   IncomeDataset\n",
      "Proxy                     |   covariance_proxy\n",
      "Effort                    |   Optimal_Effort\n",
      "alpha                     |   0.0\n",
      "lambda                    |   0.0\n",
      "delta                     |   0.5\n",
      "GD Epochs                 |   100\n",
      "GD lr                     |   0.01\n",
      "PGA Epochs                |   until convergence\n",
      "------------------------------------------------\n",
      "------------------------------------------------\n",
      "Evaluation Results\n",
      "--------------------------------------\n",
      "alpha                     |   0.0\n",
      "Accuracy                  |   0.63287\n",
      "Fairness Loss             |   0.00010\n",
      "EI Disparity              |   0.01070\n",
      "--------------------------------------\n",
      "alpha                     |   0.1\n",
      "Accuracy                  |   0.63287\n",
      "Fairness Loss             |   0.00085\n",
      "EI Disparity              |   0.07522\n",
      "--------------------------------------\n",
      "alpha                     |   0.5\n",
      "Accuracy                  |   0.63287\n",
      "Fairness Loss             |   0.00709\n",
      "EI Disparity              |   0.29136\n",
      "--------------------------------------\n",
      "alpha                     |   1.5\n",
      "Accuracy                  |   0.63287\n",
      "Fairness Loss             |   0.02170\n",
      "EI Disparity              |   0.44831\n",
      "--------------------------------------\n",
      "alpha                     |   5.0\n",
      "Accuracy                  |   0.63287\n",
      "Fairness Loss             |   0.05055\n",
      "EI Disparity              |   0.53668\n",
      "--------------------------------------\n"
     ]
    }
   ],
   "source": [
    "lf_n = 48\n",
    "alpha = alpha\n",
    "print(f'Train Parameters')\n",
    "print('-'*lf_n)\n",
    "print(f'Dataset                   |   {dataset.__class__.__name__}')\n",
    "print(f'Proxy                     |   {proxy.__name__}')\n",
    "print(f'Effort                    |   {effort.__class__.__name__}')\n",
    "print(f'alpha                     |   {alpha}')\n",
    "print(f'lambda                    |   {lamb}')\n",
    "print(f'delta                     |   {delta}')\n",
    "print(f'GD Epochs                 |   {n_epochs}')\n",
    "print(f'GD lr                     |   {lr}')\n",
    "print(f'PGA Epochs                |   until convergence')\n",
    "print('-'*lf_n)\n",
    "print('-'*lf_n)\n",
    "print('Evaluation Results')\n",
    "print('-'*38)\n",
    "for alpha_eval in [0., 0.1, 0.5, 1.5, 5.0]:\n",
    "    Y_hat, Y_hat_max, fair_loss = ei_model.predict(test_dataset, alpha_eval, 1e-7)\n",
    "    accuracy, ei_disparity = model_performance(test_dataset.Y.detach().numpy(), test_dataset.Z.detach().numpy(), Y_hat, Y_hat_max, tau)\n",
    "    for module in ei_model.model_adv.layers:\n",
    "        if hasattr(module, 'weight'):\n",
    "            weights_adv = module.weight.data\n",
    "        if hasattr(module, 'bias'):\n",
    "            bias_adv = module.bias.data\n",
    "    theta_adv = torch.cat((weights_adv[0], bias_adv), 0)\n",
    "    alphas = (theta_adv-theta_0).abs()\n",
    "\n",
    "    # Y_hat_r, Y_hat_max_r, fair_loss_r = rei_model.predict(test_dataset, alpha, 1e-7)\n",
    "    # accuracy_r, rei_disparity = model_performance(test_dataset.Y.detach().numpy(), test_dataset.Z.detach().numpy(), Y_hat_r, Y_hat_max_r, tau)\n",
    "    # for module in ei_model.model_adv.layers:\n",
    "    #     if hasattr(module, 'weight'):\n",
    "    #         weights_adv_r = module.weight.data\n",
    "    #     if hasattr(module, 'bias'):\n",
    "    #         bias_adv_r = module.bias.data\n",
    "    # theta_adv_r = torch.cat((weights_adv_r[0], bias_adv_r), 0)\n",
    "    # alphas_r = (theta_adv_r-theta_0_r).abs()\n",
    "\n",
    "    print(f'alpha                     |   {alpha_eval}')\n",
    "    print(f'Accuracy                  |   {accuracy:.5f}')\n",
    "    # print(f'Accuracy (Robust)         |   {accuracy_r:.5f}')\n",
    "    print(f'Fairness Loss             |   {fair_loss:.5f}')\n",
    "    # print(f'Fairness Loss (Robust)    |   {fair_loss_r:.5f}')\n",
    "    print(f'EI Disparity              |   {ei_disparity:.5f}')\n",
    "    # print(f'EI Disparity (Robust)     |   {rei_disparity:.5f}')\n",
    "    # print(f'alphas                    |   {np.round(alphas, 4)}')\n",
    "    # print(f'alphas (Robust)           |   {np.round(alphas_r, 4)}')\n",
    "    # print(theta_0)\n",
    "    # print(theta_adv)\n",
    "    # print(theta_adv_r)\n",
    "    print('-'*38)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
