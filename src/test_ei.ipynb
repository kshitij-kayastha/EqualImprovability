{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from model import LR\n",
    "from data import FairnessDataset, SyntheticDataset, GermanDataset, IncomeDataset\n",
    "from ei_effort import Optimal_Effort, PGD_Effort\n",
    "from ei_utils import *\n",
    "from ei_model_test import EIModel, fair_batch_proxy, covariance_proxy\n",
    "\n",
    "# warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = SyntheticDataset(seed=1)\n",
    "# dataset = GermanDataset(seed=1)\n",
    "dataset = IncomeDataset(seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training [alpha=0.00; lambda=1.00; delta=0.50]: 100%|\u001b[38;2;0;145;255m██████████\u001b[0m| 100/100 [02:22<00:00,  1.43s/epochs]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-5.0768e-02,  6.1250e-01, -2.0084e-02,  2.5392e-01,  1.5941e-06,\n",
      "        -2.0984e-08,  1.8928e-38, -4.2040e-13,  2.4220e-02,  4.4699e-41,\n",
      "        -1.0041e-04, -1.3343e-01,  1.4816e-06, -2.1034e-07,  1.7032e-03,\n",
      "         4.2090e-01,  3.5859e-04,  1.8606e-11, -4.1629e-08,  1.5001e-10,\n",
      "        -7.3839e-07,  2.7593e-40,  1.6990e-06, -4.1140e-04,  2.3347e-08,\n",
      "         1.2771e-08,  7.0285e-05, -1.1125e-03, -1.7678e-11,  1.1844e-11,\n",
      "         7.3936e-05, -2.3085e-10, -2.3214e-01, -9.6004e-13,  2.4126e-07,\n",
      "         1.4484e-04, -3.9083e-09, -2.1511e-41,  6.3041e-09, -2.1257e-08,\n",
      "        -1.5700e-11, -1.0670e-04, -2.7526e-12,  1.7122e-07, -8.3404e-08,\n",
      "        -9.8763e-07,  2.2219e-10, -9.1111e-06,  1.9132e-01, -3.4500e-12,\n",
      "        -3.4430e-09,  8.2701e-12, -2.2346e-11, -2.1782e-10,  2.4815e-07,\n",
      "        -7.8131e-09,  2.1014e-08,  2.3217e-09,  1.0620e-07, -1.2352e-06,\n",
      "         6.6183e-10, -6.0239e-14, -1.3673e-11,  7.3062e-40,  1.8677e-10,\n",
      "         2.2926e-07,  2.2274e-10,  2.2041e-08,  9.2651e-12,  4.8025e-05,\n",
      "        -1.4142e-07, -6.8055e-12, -2.0116e-12,  1.7224e-11,  1.1756e-06,\n",
      "         1.2244e-06,  1.2479e-10,  3.2004e-07, -3.3304e-09,  1.9022e-11,\n",
      "        -4.5206e-40, -5.3658e-08, -1.1559e-12, -9.0375e-11, -6.3901e-05,\n",
      "         2.0516e-08, -3.2441e-08,  5.2976e-10,  1.7524e-08,  2.6713e-11,\n",
      "         1.5359e-08,  6.4824e-07,  7.7178e-06, -5.5448e-10, -4.3587e-40,\n",
      "         2.2459e-09, -4.1243e-08, -1.2335e-08, -3.7119e-13,  3.4262e-13,\n",
      "        -3.3612e-15,  9.5900e-11, -1.0668e-11,  1.2593e-11, -1.2650e-07,\n",
      "        -1.7961e-13, -3.7356e-04, -1.6260e-13,  1.6951e-11,  2.0439e-12,\n",
      "         3.5426e-08, -1.1021e-12, -6.1963e-13,  1.4620e-07,  5.8141e-06,\n",
      "        -7.2335e-42,  2.0275e-09, -3.4451e-07, -2.3076e-12,  4.5442e-04,\n",
      "        -3.3702e-05, -2.4912e-07, -3.2014e-04,  8.4819e-10,  5.7998e-04,\n",
      "        -4.5417e-04,  1.8534e-07, -5.7703e-09,  3.9952e-08, -1.5270e-11,\n",
      "        -2.7017e-12,  1.0668e-11,  2.1235e-08, -3.8560e-10,  1.0914e-05,\n",
      "        -7.4826e-10,  4.0469e-40, -2.2429e-07,  3.4791e-09,  6.0702e-11,\n",
      "         9.4098e-14, -1.3284e-10,  4.8352e-05,  3.2776e-40,  1.9672e-06,\n",
      "         2.6859e-12,  8.0633e-08,  2.1553e-04,  1.0039e-08,  6.0053e-11,\n",
      "         8.2363e-06,  1.6182e-11, -3.1791e-10,  7.8147e-04,  9.7791e-14,\n",
      "        -4.0916e-13,  6.1822e-04,  1.4612e-12,  1.3082e-04, -1.3587e-06,\n",
      "        -1.9551e-05,  2.0848e-04,  2.4676e-07, -3.8769e-09,  3.0265e-10,\n",
      "        -4.9031e-05, -1.0473e-12, -2.2503e-05,  9.7137e-06,  1.4331e-05,\n",
      "         6.7476e-08, -1.0961e-13, -2.8405e-05,  5.9513e-40,  4.0490e-06,\n",
      "        -4.1575e-10, -9.0150e-09, -1.2903e-11,  3.7796e-09,  6.9819e-06,\n",
      "        -4.5330e-13,  1.0802e-12, -9.5701e-11, -3.0165e-05,  9.2876e-07,\n",
      "         2.8036e-07, -8.9920e-08,  2.8301e-07, -5.3203e-13, -2.0418e-13,\n",
      "         9.0608e-14, -7.4989e-06,  6.9012e-40,  1.8720e-13,  6.6509e-10,\n",
      "        -9.9393e-07, -1.0201e-08, -7.0311e-11, -5.0210e-40,  5.3288e-07,\n",
      "         8.2342e-08, -6.4466e-40, -3.6897e-05, -1.2769e-07, -1.9326e-13,\n",
      "        -1.4360e-11, -2.0374e-08,  4.6787e-05, -8.2288e-10,  6.4109e-13,\n",
      "         3.2690e-08, -1.3815e-11, -4.8973e-09, -9.2897e-12,  8.5530e-06,\n",
      "         2.2028e-09,  1.8378e-10,  2.3390e-05, -1.5737e-12,  1.4324e-10,\n",
      "         1.7162e-07, -1.6247e-11, -9.9905e-08, -2.1177e-07,  8.1214e-09,\n",
      "        -7.2086e-06, -9.8445e-06, -3.2850e-12,  1.0965e-10,  1.3286e-12,\n",
      "         1.5863e-08,  3.1368e-04,  7.6441e-04,  8.2605e-05, -1.3119e-06,\n",
      "        -1.2456e-06,  3.7129e-12, -6.2842e-13,  2.9143e-12,  2.6547e-01,\n",
      "        -3.2325e-04,  1.9929e-09, -1.1788e-11,  3.0930e-11, -1.9280e-07,\n",
      "        -6.7098e-13, -1.8703e-01,  6.3005e-12,  4.6434e-12, -8.9460e-07,\n",
      "         4.3210e-05, -2.9730e-08,  8.4194e-09, -2.1339e-13, -3.6093e-13,\n",
      "         7.7506e-42,  3.5020e-11,  8.1052e-07, -1.9973e-10,  1.6499e-10,\n",
      "        -6.9933e-13,  2.2007e-09,  3.4665e-12,  1.5454e-40, -7.3982e-40,\n",
      "         5.1683e-13, -1.5705e-06,  2.8349e-12, -2.0878e-09, -2.4471e-06,\n",
      "         6.7268e-41, -5.9296e-09,  7.2186e-05,  7.9479e-08,  6.8213e-40,\n",
      "         8.4201e-07,  1.3951e-06,  8.1724e-11, -1.2638e-11, -5.3127e-13,\n",
      "         2.2334e-12,  1.9018e-04, -5.3060e-41,  1.4947e-10, -1.3678e-40,\n",
      "         4.5660e-08, -5.9011e-12, -2.7546e-21,  1.8181e-10, -1.0377e-10,\n",
      "         3.8598e-13, -5.0035e-10, -2.1589e-08,  3.8511e-09, -6.1211e-11,\n",
      "        -1.0084e-09,  4.8243e-05,  1.8313e-13,  1.1729e-06, -1.7665e-40,\n",
      "         9.9284e-07,  3.2980e-11, -4.6693e-08,  1.5613e-09,  3.1822e-07,\n",
      "        -2.1806e-40,  1.0472e-09, -2.3745e-11, -7.2884e-05,  2.7153e-12,\n",
      "         1.2424e-36,  2.3360e-09, -1.5271e-40,  1.3283e-07, -1.0111e-06,\n",
      "        -2.4412e-40, -5.8711e-14, -4.9823e-13, -1.0490e-08, -1.1060e-09,\n",
      "         7.0284e-10,  7.4590e-09, -1.4910e-12,  1.9261e-08, -3.2371e-12,\n",
      "         6.3513e-05, -8.9191e-08, -4.6915e-13,  5.2539e-09,  8.5615e-10,\n",
      "         1.7597e-07, -1.1750e-06, -1.2620e-05, -9.7953e-14, -8.8645e-12,\n",
      "        -1.8895e-08, -9.9869e-12, -5.1204e-08, -8.2823e-06,  1.8374e-12,\n",
      "         1.6025e-03, -2.5016e-07,  3.3090e-11, -5.5964e-05,  1.1643e-08,\n",
      "        -2.7825e-09,  9.9332e-07,  1.2277e-05,  1.0887e-11, -3.9397e-05,\n",
      "         5.6704e-04,  1.5120e-11, -6.7731e-11,  2.4891e-09, -2.9207e-07,\n",
      "        -2.4567e-10, -7.1677e-40,  1.9799e-09,  2.7951e-12, -3.2365e-11,\n",
      "        -9.3365e-14,  1.0495e-13, -2.0791e-08,  4.1004e-11,  1.9953e-06,\n",
      "        -2.8397e-04,  2.6710e-06,  7.0123e-08, -6.2619e-06, -3.8370e-08,\n",
      "         5.3300e-08, -3.7309e-06, -4.0489e-12, -2.9059e-10,  2.9489e-04,\n",
      "        -1.7589e-10, -1.7841e-09, -8.3880e-12,  1.6656e-05,  1.0850e-05,\n",
      "         5.0242e-05,  8.1525e-03, -1.2861e-13,  6.1127e-13, -7.0668e-40,\n",
      "         3.4141e-09,  1.9528e-06,  1.1996e-40, -2.9422e-04, -2.1731e-06,\n",
      "        -2.4483e-12,  5.1466e-18,  2.9897e-12,  9.1070e-12, -1.1419e-06,\n",
      "        -7.4275e-11,  2.8515e-06, -1.1124e-09, -6.2802e-12, -7.1727e-06,\n",
      "         4.8988e-08,  1.3782e-12, -9.8930e-14, -3.5090e-05,  2.5073e-40,\n",
      "        -6.3014e-13,  8.4438e-13,  2.7201e-13,  2.6042e-07, -1.3206e-12,\n",
      "         1.0957e-12,  5.0286e-06,  5.8184e-14,  2.2519e-06, -1.4363e-13,\n",
      "        -3.5232e-04,  2.2660e-06, -6.7301e-14, -2.6666e-10, -3.6435e-08,\n",
      "         1.3290e-10, -2.2428e-40,  1.5018e-06, -7.9912e-07, -8.7981e-09,\n",
      "        -1.7649e-04, -2.3665e-11,  1.5022e-06,  3.1803e-05,  1.4722e-21,\n",
      "         5.7278e-05, -2.8466e-07,  5.0059e-08, -6.0346e-06,  1.8799e-06,\n",
      "        -1.2416e-07,  2.3216e-10,  4.3545e-11, -1.4288e-01,  7.3050e-40,\n",
      "        -6.2667e-14, -2.3730e-11,  1.1977e-12, -2.1363e-13,  6.4683e-04,\n",
      "         5.1073e-05,  1.4905e-04,  1.3809e-06,  1.3752e-10, -5.7110e-07,\n",
      "        -4.6854e-08, -8.0328e-05,  5.8682e-40, -4.8314e-41,  3.9440e-07,\n",
      "         3.1601e-10, -9.3151e-08,  2.7483e-31, -1.4338e-40,  4.2066e-12,\n",
      "         2.4348e-05, -7.3471e-08,  5.1710e-04, -1.7036e-40,  2.7330e-11,\n",
      "         6.5362e-04,  1.0255e-12, -4.4721e-40,  2.3863e-13,  3.9318e-13,\n",
      "         1.7416e-06,  4.6844e-40, -1.0237e-09,  1.2838e-09,  1.7644e-01,\n",
      "         9.1338e-04,  1.3933e-01,  5.3469e-09, -9.5235e-04, -2.3452e-08,\n",
      "        -7.1215e-12,  1.0642e-04,  1.4225e-10,  5.1120e-08,  3.8469e-14,\n",
      "        -1.4016e-05, -3.8597e-12, -7.0507e-13,  2.4519e-10, -2.0967e-15,\n",
      "        -1.8954e-12,  5.8898e-07,  9.3170e-08,  8.7150e-12, -3.1378e-10,\n",
      "         2.3740e-12,  1.9664e-10, -2.8576e-13, -2.5593e-04,  1.8599e-06,\n",
      "         7.8450e-12, -1.0515e-05,  1.8829e-09, -1.8648e-09,  5.3469e-05,\n",
      "        -4.7904e-08,  5.2466e-10, -3.4065e-07,  3.9645e-40, -5.5568e-10,\n",
      "         2.6209e-10, -6.4904e-40,  4.1032e-07,  5.6706e-06,  1.8677e-04,\n",
      "        -3.1768e-10,  4.3503e-41,  1.5451e-07, -3.1355e-12, -1.3239e-11,\n",
      "         1.0301e-09,  1.9684e-13,  1.6800e-13, -6.2110e-40, -8.9111e-14,\n",
      "        -2.1424e-07,  3.4446e-13, -5.5430e-05,  3.0523e-11,  2.2655e-09,\n",
      "         7.7067e-10,  4.7688e-05, -6.2299e-13, -2.1542e-18, -1.2129e-01,\n",
      "         4.1870e-11, -1.2869e-07, -5.0726e-08,  1.5880e-10, -4.6443e-06,\n",
      "         4.6121e-40,  3.3098e-09,  4.6398e-11,  6.4477e-10, -1.2342e-10,\n",
      "         1.9830e-10, -1.0867e-06,  1.1719e-12,  5.2731e-13,  1.5530e-02,\n",
      "         2.8694e-07, -2.4168e-08,  1.5806e-04,  5.0291e-41, -3.1053e-12,\n",
      "         2.3409e-08, -1.3703e-03,  1.0213e-13, -1.0961e-09,  5.7797e-14,\n",
      "        -5.6888e-12, -1.7981e-40, -4.7366e-08, -2.6779e-09, -2.6049e-08,\n",
      "        -6.5887e-09, -6.8220e-04,  3.6861e-11,  1.0040e-12, -3.4448e-13,\n",
      "         4.5039e-06,  2.2750e-08, -3.9411e-12,  4.0382e-09,  1.6045e-06,\n",
      "        -5.3558e-12,  2.4386e-09,  4.8608e-06, -5.5364e-05, -1.8906e-10,\n",
      "         8.4593e-10, -7.0096e-10,  1.2417e-04,  4.1325e-09,  8.4396e-08,\n",
      "         9.2454e-05, -5.0930e-06,  7.0997e-05,  1.9123e-01, -1.1693e-05,\n",
      "        -3.5521e-12, -6.3735e-09, -5.3452e-09, -6.6401e-06,  4.9629e-40,\n",
      "         1.3249e-13, -1.2345e-04, -1.0807e-04, -3.4789e-40, -5.8667e-07,\n",
      "         1.3997e-08, -3.0425e-06,  1.9081e-07, -3.1399e-41,  7.6653e-10,\n",
      "         3.7857e-12,  4.7606e-04,  2.4089e-06,  5.4908e-07,  2.6499e-09,\n",
      "        -3.0728e-05, -1.6170e-04, -2.6501e-40,  3.9843e-09,  1.0080e-06,\n",
      "        -9.4547e-11, -5.4656e-05,  1.2938e-07,  9.3482e-09, -1.4098e-05,\n",
      "        -3.6835e-12,  1.7659e-12, -8.4521e-13, -8.9523e-13,  2.0872e-05,\n",
      "        -9.4732e-06, -1.0119e-06, -8.5434e-10, -2.9568e-07,  1.7137e-07,\n",
      "         1.4375e-40,  2.8148e-04,  1.1355e-10,  6.5167e-13,  3.5312e-04,\n",
      "        -7.8234e-09, -2.3587e-12, -1.7285e-07,  2.3672e-04,  4.2894e-11,\n",
      "         6.4679e-12, -9.8435e-08, -6.0948e-08, -5.4488e-06, -6.2132e-07,\n",
      "         9.3551e-06, -3.8172e-14, -5.3176e-10,  5.7887e-40,  1.0464e-09,\n",
      "        -3.2227e-13,  1.7799e-24, -1.7526e-20, -1.8670e-07,  2.5837e-41,\n",
      "         1.4970e-10, -6.2226e-13, -1.4681e-06,  2.3311e-07,  7.1952e-11,\n",
      "        -3.0233e-05, -7.4111e-13,  5.5130e-10,  3.2103e-09,  1.8842e-13,\n",
      "         1.4272e-14, -6.4551e-13, -8.7723e-10, -4.9887e-09,  4.5310e-41,\n",
      "         1.0228e-13,  5.7015e-10,  5.7997e-13,  1.6761e-08, -9.0301e-12,\n",
      "        -2.5963e-12, -3.0640e-12, -1.1226e-12,  2.3242e-40,  5.1445e-05,\n",
      "        -1.1598e-09,  4.0066e-28, -6.0842e-13, -5.1341e-13, -2.5820e-10,\n",
      "        -2.0580e-09, -1.4566e-10,  3.8556e-06,  6.8801e-02,  4.7136e-13,\n",
      "        -6.6126e-05, -2.0454e-07,  1.3933e-01,  1.7644e-01, -1.9615e-10,\n",
      "         4.9979e-06,  7.6001e-05, -3.2321e-04, -7.5736e-13,  8.0960e-08,\n",
      "        -6.0861e-12, -5.4936e-04,  2.1934e-07,  6.9064e-10,  8.8541e-11,\n",
      "        -2.6415e-12,  1.1462e-05, -8.4178e-11,  4.6151e-11,  4.8093e-07,\n",
      "         1.2935e-11, -2.0673e-06, -4.7603e-08,  2.9922e-06,  8.3013e-14,\n",
      "         1.0532e-05,  1.0640e-09,  2.3006e-06,  4.8042e-08, -2.4123e-40,\n",
      "         8.8393e-11, -4.6125e-07,  2.0026e-08, -1.3159e-12, -3.5554e-11,\n",
      "        -1.1291e-06,  3.0308e-07, -1.4415e-13,  2.1561e-08,  5.9670e-05,\n",
      "         8.4049e-13,  1.7577e-09, -3.6065e-11,  6.0997e-06, -1.3709e-04,\n",
      "        -1.4106e-03, -2.5101e-10,  3.5395e-05, -5.0592e-13, -4.3455e-12,\n",
      "        -4.6168e-11,  2.1402e-08, -1.5082e-08, -1.9005e-01,  1.7692e-10,\n",
      "        -1.7678e-07,  1.8812e-08,  2.1799e-04, -6.0791e-09,  3.0834e-06,\n",
      "         3.2430e-06, -3.7229e-05,  8.2961e-12, -4.4664e-10, -9.3039e-09,\n",
      "        -4.3256e-09,  1.4404e-12, -8.5283e-14,  7.8434e-07,  1.9858e-07,\n",
      "        -9.2663e-11, -1.4829e-03, -6.3776e-06,  9.1870e-10, -3.6224e-02,\n",
      "         1.7644e-01,  1.8889e-08, -9.5720e-08, -9.2006e-10, -8.0555e-12,\n",
      "        -7.0934e-36,  5.9491e-09, -2.1806e-40, -3.5826e-04,  1.9123e-01,\n",
      "        -2.4123e-40, -2.4888e-11, -1.6860e-01, -1.4432e-12,  5.3415e-14,\n",
      "        -1.7312e-06, -2.0774e-09, -5.2811e-02,  1.8954e-38,  2.2666e-11,\n",
      "        -2.7214e-01, -2.7214e-01,  2.7008e-01,  2.7214e-01,  4.1365e-01,\n",
      "        -2.6756e-01,  3.2143e-01,  6.1258e-01])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "train_tensors, val_tensors, test_tensors = dataset.tensor(z_blind=False)\n",
    "train_dataset = FairnessDataset(*train_tensors, dataset.imp_feats)\n",
    "val_dataset = FairnessDataset(*val_tensors, dataset.imp_feats)\n",
    "test_dataset = FairnessDataset(*test_tensors, dataset.imp_feats)\n",
    "\n",
    "model = LR(train_dataset.X.shape[1])\n",
    "\n",
    "tau = 0.5\n",
    "delta = 0.5\n",
    "lamb = 1.\n",
    "alpha = 0.\n",
    "effort = Optimal_Effort(delta)\n",
    "proxy = fair_batch_proxy\n",
    "lr = 1e-2\n",
    "n_epochs = 100\n",
    "\n",
    "# EI Model\n",
    "ei_model = EIModel(deepcopy(model), proxy, effort, tau)\n",
    "ei_model.train(train_dataset, lamb, alpha, lr, n_epochs)\n",
    "for module in ei_model.model.layers:\n",
    "    if hasattr(module, 'weight'):\n",
    "        weights_0 = module.weight.data\n",
    "    if hasattr(module, 'bias'):\n",
    "        bias_0 = module.bias.data\n",
    "theta_0 = torch.cat((weights_0[0], bias_0), 0)\n",
    "print(theta_0)\n",
    "\n",
    "# # REI Model\n",
    "# rei_model = EIModel(deepcopy(model), proxy, effort, tau)\n",
    "# rei_model.train(train_dataset, lamb, alpha, n_epochs=200, abstol=1e-7)\n",
    "# for module in rei_model.model.layers:\n",
    "#     if hasattr(module, 'weight'):\n",
    "#         weights_r_0 = module.weight.data\n",
    "#     if hasattr(module, 'bias'):\n",
    "#         bias_r_0 = module.bias.data\n",
    "# theta_0_r = torch.cat((weights_r_0[0], bias_r_0), 0)\n",
    "\n",
    "# print(theta_0_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9999, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ei_model.model(test_dataset.X).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Parameters\n",
      "------------------------------------------------\n",
      "Dataset                   |   IncomeDataset\n",
      "Proxy                     |   fair_batch_proxy\n",
      "Effort                    |   Optimal_Effort\n",
      "alpha                     |   0.0\n",
      "lambda                    |   1.0\n",
      "delta                     |   0.5\n",
      "GD Epochs                 |   100\n",
      "GD lr                     |   0.01\n",
      "PGA Epochs                |   until convergence\n",
      "------------------------------------------------\n",
      "------------------------------------------------\n",
      "Evaluation Results\n",
      "--------------------------------------\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m38\u001b[39m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m alpha_eval \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m0.\u001b[39m, \u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m1.5\u001b[39m, \u001b[38;5;241m5.0\u001b[39m]:\n\u001b[0;32m---> 19\u001b[0m     Y_hat, Y_hat_max, fair_loss \u001b[38;5;241m=\u001b[39m \u001b[43mei_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha_eval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1e-7\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m     accuracy, ei_disparity \u001b[38;5;241m=\u001b[39m model_performance(test_dataset\u001b[38;5;241m.\u001b[39mY\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy(), test_dataset\u001b[38;5;241m.\u001b[39mZ\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy(), Y_hat, Y_hat_max, tau)\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m ei_model\u001b[38;5;241m.\u001b[39mmodel_adv\u001b[38;5;241m.\u001b[39mlayers:\n",
      "File \u001b[0;32m~/School/PhD/Research/EqualImprovability/src/ei_model_test.py:191\u001b[0m, in \u001b[0;36mEIModel.predict\u001b[0;34m(self, dataset, alpha, abstol)\u001b[0m\n\u001b[1;32m    188\u001b[0m fair_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproxy(Z_e, Y_hat_max, pga_loss_fn)\n\u001b[1;32m    190\u001b[0m optimizer_adv\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 191\u001b[0m \u001b[43mfair_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    192\u001b[0m optimizer_adv\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    194\u001b[0m loss_diff \u001b[38;5;241m=\u001b[39m (prev_loss \u001b[38;5;241m-\u001b[39m fair_loss)\u001b[38;5;241m.\u001b[39mabs()\n",
      "File \u001b[0;32m~/School/PhD/Research/EqualImprovability/.env/lib/python3.11/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/School/PhD/Research/EqualImprovability/.env/lib/python3.11/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "lf_n = 48\n",
    "alpha = alpha\n",
    "print(f'Train Parameters')\n",
    "print('-'*lf_n)\n",
    "print(f'Dataset                   |   {dataset.__class__.__name__}')\n",
    "print(f'Proxy                     |   {proxy.__name__}')\n",
    "print(f'Effort                    |   {effort.__class__.__name__}')\n",
    "print(f'alpha                     |   {alpha}')\n",
    "print(f'lambda                    |   {lamb}')\n",
    "print(f'delta                     |   {delta}')\n",
    "print(f'GD Epochs                 |   {n_epochs}')\n",
    "print(f'GD lr                     |   {lr}')\n",
    "print(f'PGA Epochs                |   until convergence')\n",
    "print('-'*lf_n)\n",
    "print('-'*lf_n)\n",
    "print('Evaluation Results')\n",
    "print('-'*38)\n",
    "for alpha_eval in [0., 0.1, 0.5, 1.5, 5.0]:\n",
    "    Y_hat, Y_hat_max, fair_loss = ei_model.predict(test_dataset, alpha_eval, 1e-7)\n",
    "    accuracy, ei_disparity = model_performance(test_dataset.Y.detach().numpy(), test_dataset.Z.detach().numpy(), Y_hat, Y_hat_max, tau)\n",
    "    for module in ei_model.model_adv.layers:\n",
    "        if hasattr(module, 'weight'):\n",
    "            weights_adv = module.weight.data\n",
    "        if hasattr(module, 'bias'):\n",
    "            bias_adv = module.bias.data\n",
    "    theta_adv = torch.cat((weights_adv[0], bias_adv), 0)\n",
    "    alphas = (theta_adv-theta_0).abs()\n",
    "\n",
    "    # Y_hat_r, Y_hat_max_r, fair_loss_r = rei_model.predict(test_dataset, alpha, 1e-7)\n",
    "    # accuracy_r, rei_disparity = model_performance(test_dataset.Y.detach().numpy(), test_dataset.Z.detach().numpy(), Y_hat_r, Y_hat_max_r, tau)\n",
    "    # for module in ei_model.model_adv.layers:\n",
    "    #     if hasattr(module, 'weight'):\n",
    "    #         weights_adv_r = module.weight.data\n",
    "    #     if hasattr(module, 'bias'):\n",
    "    #         bias_adv_r = module.bias.data\n",
    "    # theta_adv_r = torch.cat((weights_adv_r[0], bias_adv_r), 0)\n",
    "    # alphas_r = (theta_adv_r-theta_0_r).abs()\n",
    "\n",
    "    print(f'alpha                     |   {alpha_eval}')\n",
    "    print(f'Accuracy                  |   {accuracy:.5f}')\n",
    "    # print(f'Accuracy (Robust)         |   {accuracy_r:.5f}')\n",
    "    print(f'Fairness Loss             |   {fair_loss:.5f}')\n",
    "    # print(f'Fairness Loss (Robust)    |   {fair_loss_r:.5f}')\n",
    "    print(f'EI Disparity              |   {ei_disparity:.5f}')\n",
    "    # print(f'EI Disparity (Robust)     |   {rei_disparity:.5f}')\n",
    "    # print(f'alphas                    |   {np.round(alphas, 4)}')\n",
    "    # print(f'alphas (Robust)           |   {np.round(alphas_r, 4)}')\n",
    "    # print(theta_0)\n",
    "    # print(theta_adv)\n",
    "    # print(theta_adv_r)\n",
    "    print('-'*38)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
