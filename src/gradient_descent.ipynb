{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "\n",
    "from model import LR\n",
    "from data import FairnessDataset, Dataset, SyntheticDataset\n",
    "from ei_effort import Optimal_Effort\n",
    "from ei_utils import model_performance, pdump\n",
    "from ei_model import EIModel, fair_batch_proxy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_layout = lambda width = 720, height = 540, color='#5d5d5d': dict(\n",
    "    width = width,\n",
    "    height = height,\n",
    "    font=dict(\n",
    "        family='Iosevka', \n",
    "        color=color\n",
    "        ),\n",
    "    title=dict(\n",
    "        x=0.5,\n",
    "        font=dict(size=17), \n",
    "        ),\n",
    "    legend=dict(\n",
    "        font=dict(size=10),\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_res(d, seed, method, delta, lamb, train_alpha, alpha, pred_loss, fair_loss, accuracy, ei_disparity, rei_disparity, ei_model):\n",
    "    d['seed'].append(seed)\n",
    "    d['method'].append(method)\n",
    "    d['delta'].append(delta)\n",
    "    d['lambda'].append(lamb)\n",
    "    d['train_alpha'].append(train_alpha)\n",
    "    d['alpha'].append(alpha)\n",
    "    d['loss'].append((1-lamb)*pred_loss + lamb*fair_loss)\n",
    "    d['pred_loss'].append(pred_loss)\n",
    "    d['fair_loss'].append(fair_loss)\n",
    "    d['accuracy'].append(accuracy)\n",
    "    d['error'].append(1-accuracy)\n",
    "    d['ei_disparity'].append(ei_disparity)\n",
    "    d['rei_disparity'].append(rei_disparity)\n",
    "    d['ei_model'].append(ei_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_tradeoff(dataset: Dataset, params: dict, seeds: int, results: dict):\n",
    "    \n",
    "    tau = params['tau']\n",
    "    delta = dataset.delta\n",
    "    \n",
    "    for seed in seeds:\n",
    "        train_tensors, val_tensors, test_tensors = dataset.tensor(fold=seed)\n",
    "        train_dataset = FairnessDataset(*train_tensors, dataset.imp_feats)\n",
    "        val_dataset = FairnessDataset(*val_tensors, dataset.imp_feats)\n",
    "        test_dataset = FairnessDataset(*test_tensors, dataset.imp_feats)\n",
    "        \n",
    "        model = LR(num_features=train_dataset.X.shape[1])\n",
    "        \n",
    "        for lamb in params['lambda']:\n",
    "            ei_model = EIModel(\n",
    "                    model = deepcopy(model).xavier_init(),\n",
    "                    proxy = params['proxy'],\n",
    "                    effort = params['effort'],\n",
    "                    tau = params['tau']\n",
    "                    )\n",
    "                \n",
    "            ei_model.train(\n",
    "                train_dataset,\n",
    "                lamb=lamb,\n",
    "                alpha=0.,\n",
    "                lr=params['learning_rate'],\n",
    "                n_epochs=params['n_epochs'],\n",
    "                batch_size=params['batch_size'],\n",
    "                abstol=params['pga_abstol'],\n",
    "                pga_n_iters=params['pga_n_iters']\n",
    "                )\n",
    "            \n",
    "            Y_hat, Y_hat_max, pred_loss, fair_loss = ei_model.predict(test_dataset, alpha=0., abstol=params['pga_abstol'])\n",
    "            accuracy, ei_disparity = model_performance(test_dataset.Y.detach().numpy(), test_dataset.Z[(Y_hat<tau)].detach().numpy(), Y_hat, Y_hat_max, tau)\n",
    "            \n",
    "            for alpha in params['alpha']:\n",
    "                Y_hat, Y_hat_max, pred_loss, fair_loss = ei_model.predict(test_dataset, alpha=alpha, abstol=params['pga_abstol'], pga_n_iters=params['pga_n_iters'])\n",
    "                accuracy, rei_disparity = model_performance(test_dataset.Y.detach().numpy(), test_dataset.Z[(Y_hat<tau)].detach().numpy(), Y_hat, Y_hat_max, tau)\n",
    "                append_res(results, seed, 'EI', delta, lamb, 0., alpha, pred_loss, fair_loss, accuracy, ei_disparity, rei_disparity, deepcopy(ei_model))\n",
    "                \n",
    "                rei_model = EIModel(\n",
    "                    model = deepcopy(model),\n",
    "                    proxy = params['proxy'],\n",
    "                    effort = params['effort'],\n",
    "                    tau = params['tau']\n",
    "                    )\n",
    "                \n",
    "                rei_model.train(\n",
    "                    train_dataset,\n",
    "                    lamb=lamb,\n",
    "                    alpha=alpha,\n",
    "                    lr=params['learning_rate'],\n",
    "                    n_epochs=params['n_epochs'],\n",
    "                    batch_size=params['batch_size'],\n",
    "                    abstol=params['pga_abstol'],\n",
    "                    pga_n_iters=params['pga_n_iters']\n",
    "                    )\n",
    "                \n",
    "                Y_hat, Y_hat_max, pred_loss, fair_loss = rei_model.predict(test_dataset, alpha=0., abstol=params['pga_abstol'])\n",
    "                accuracy, ei_disparity = model_performance(test_dataset.Y.detach().numpy(), test_dataset.Z[(Y_hat<tau)].detach().numpy(), Y_hat, Y_hat_max, tau)\n",
    "                \n",
    "                Y_hat_r, Y_hat_max_r, pred_loss_r, fair_loss_r = rei_model.predict(test_dataset, alpha=alpha, abstol=params['pga_abstol'], pga_n_iters=params['pga_n_iters'])\n",
    "                accuracy_r, rei_disparity = model_performance(test_dataset.Y.detach().numpy(), test_dataset.Z[(Y_hat<tau)].detach().numpy(), Y_hat_r, Y_hat_max_r, tau)\n",
    "                append_res(results, seed, 'REI', delta, lamb, alpha, alpha, pred_loss_r, fair_loss_r, accuracy_r, ei_disparity, rei_disparity, deepcopy(rei_model))\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training [alpha=0.000; lambda=0.99900; delta=0.500]: 100%|\u001b[38;2;0;145;255m██████████\u001b[0m| 100/100 [00:01<00:00, 74.11epochs/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05738085135817528\n",
      "tensor(0.0005, grad_fn=<AbsBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training [alpha=1.000; lambda=0.99900; delta=0.500]:  65%|\u001b[38;2;0;145;255m██████▌   \u001b[0m| 65/100 [00:06<00:03, 10.16epochs/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# ----- Run Experiment -----\u001b[39;00m\n\u001b[1;32m     22\u001b[0m results_xl \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m'\u001b[39m: [], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmethod\u001b[39m\u001b[38;5;124m'\u001b[39m: [], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdelta\u001b[39m\u001b[38;5;124m'\u001b[39m: [], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlambda\u001b[39m\u001b[38;5;124m'\u001b[39m: [], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_alpha\u001b[39m\u001b[38;5;124m'\u001b[39m: [], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124malpha\u001b[39m\u001b[38;5;124m'\u001b[39m: [], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m: [], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpred_loss\u001b[39m\u001b[38;5;124m'\u001b[39m: [ ], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfair_loss\u001b[39m\u001b[38;5;124m'\u001b[39m: [], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m: [], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m'\u001b[39m: [], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mei_disparity\u001b[39m\u001b[38;5;124m'\u001b[39m: [], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrei_disparity\u001b[39m\u001b[38;5;124m'\u001b[39m: [], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mei_model\u001b[39m\u001b[38;5;124m'\u001b[39m: []}\n\u001b[0;32m---> 23\u001b[0m \u001b[43mrun_tradeoff\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseeds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresults_xl\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 48\u001b[0m, in \u001b[0;36mrun_tradeoff\u001b[0;34m(dataset, params, seeds, results)\u001b[0m\n\u001b[1;32m     39\u001b[0m append_res(results, seed, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEI\u001b[39m\u001b[38;5;124m'\u001b[39m, delta, lamb, \u001b[38;5;241m0.\u001b[39m, alpha, pred_loss, fair_loss, accuracy, ei_disparity, rei_disparity, deepcopy(ei_model))\n\u001b[1;32m     41\u001b[0m rei_model \u001b[38;5;241m=\u001b[39m EIModel(\n\u001b[1;32m     42\u001b[0m     model \u001b[38;5;241m=\u001b[39m deepcopy(model),\n\u001b[1;32m     43\u001b[0m     proxy \u001b[38;5;241m=\u001b[39m params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproxy\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     44\u001b[0m     effort \u001b[38;5;241m=\u001b[39m params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meffort\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     45\u001b[0m     tau \u001b[38;5;241m=\u001b[39m params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtau\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     46\u001b[0m     )\n\u001b[0;32m---> 48\u001b[0m \u001b[43mrei_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlamb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlamb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m    \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlearning_rate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mn_epochs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbatch_size\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m    \u001b[49m\u001b[43mabstol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpga_abstol\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpga_n_iters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpga_n_iters\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m Y_hat, Y_hat_max, pred_loss, fair_loss \u001b[38;5;241m=\u001b[39m rei_model\u001b[38;5;241m.\u001b[39mpredict(test_dataset, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.\u001b[39m, abstol\u001b[38;5;241m=\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpga_abstol\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     60\u001b[0m accuracy, ei_disparity \u001b[38;5;241m=\u001b[39m model_performance(test_dataset\u001b[38;5;241m.\u001b[39mY\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy(), test_dataset\u001b[38;5;241m.\u001b[39mZ[(Y_hat\u001b[38;5;241m<\u001b[39mtau)]\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy(), Y_hat, Y_hat_max, tau)\n",
      "File \u001b[0;32m~/PhD/Research/REI/tests/../src/methods.py:100\u001b[0m, in \u001b[0;36mEIModel.train\u001b[0;34m(self, dataset, lamb, alpha, lr, n_epochs, batch_size, abstol, pga_n_iters)\u001b[0m\n\u001b[1;32m     97\u001b[0m fair_loss_pga \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproxy(Z_batch_e, Y_hat_max_pga)\n\u001b[1;32m     99\u001b[0m optimizer_adv\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 100\u001b[0m \u001b[43mfair_loss_pga\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m optimizer_adv\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    103\u001b[0m loss_diff_pga \u001b[38;5;241m=\u001b[39m (prev_loss \u001b[38;5;241m-\u001b[39m fair_loss_pga)\u001b[38;5;241m.\u001b[39mabs()\n",
      "File \u001b[0;32m~/PhD/Research/REI/.env/lib/python3.12/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PhD/Research/REI/.env/lib/python3.12/site-packages/torch/autograd/__init__.py:183\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (tensors,)\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(tensors)\n\u001b[0;32m--> 183\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbackward\u001b[39m(\n\u001b[1;32m    184\u001b[0m     tensors: _TensorOrTensors,\n\u001b[1;32m    185\u001b[0m     grad_tensors: Optional[_TensorOrTensors] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    186\u001b[0m     retain_graph: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    187\u001b[0m     create_graph: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    188\u001b[0m     grad_variables: Optional[_TensorOrTensors] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    189\u001b[0m     inputs: Optional[_TensorOrTensorsOrGradEdge] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    190\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    191\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Computes the sum of gradients of given tensors with respect to graph\u001b[39;00m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;124;03m    leaves.\u001b[39;00m\n\u001b[1;32m    193\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;124;03m            were used to compute the :attr:`tensors`.\u001b[39;00m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "# ----- Dataset -----\n",
    "dataset = SyntheticDataset(num_samples=1000, seed=0)\n",
    "\n",
    "# ----- Hyperparameters -----\n",
    "params = {}\n",
    "params['lambda'] = 1-np.geomspace(0.001, 0.999, 20)\n",
    "params['alpha'] = [1.]\n",
    "params['tau'] = 0.5\n",
    "params['learning_rate'] = 0.001\n",
    "params['n_epochs'] = 500\n",
    "params['batch_size'] = 64\n",
    "params['proxy'] = fair_batch_proxy\n",
    "params['pga_abstol'] = 1e-7\n",
    "params['pga_n_iters'] = 50\n",
    "params['effort'] = Optimal_Effort(dataset.delta)\n",
    "\n",
    "seeds = range(1)\n",
    "\n",
    "# ----- Run Experiment -----\n",
    "results_xl = {'seed': [], 'method': [], 'delta': [], 'lambda': [], 'train_alpha': [], 'alpha': [], 'loss': [], 'pred_loss': [ ], 'fair_loss': [], 'accuracy': [], 'error': [], 'ei_disparity': [], 'rei_disparity': [], 'ei_model': []}\n",
    "run_tradeoff(dataset, params, seeds, results_xl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdump(f'../results/gradientdescent/lr_synthetic_fb_alpha{params[\"alpha\"][0]}_d1_sqloss.pkl', results_xl) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['method', 'lambda', 'alpha', 'loss', 'pred_loss', 'fair_loss', 'error', 'ei_disparity', 'rei_disparity', 'theta']\n",
    "df_im = pd.DataFrame(results_xl)\n",
    "df_im['theta'] = df_im['ei_model'].apply(lambda model: model.model.get_theta().numpy().round(2))\n",
    "df_im['theta_adv'] = df_im['ei_model'].apply(lambda model: model.model_adv.get_theta().numpy().round(2))\n",
    "for i in range(0, len(df_im), 2):\n",
    "    display(df_im[columns].iloc[i:i+2].style.highlight_min(subset=['fair_loss', 'error', 'ei_disparity', 'rei_disparity'], color='#D35400'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df_im)):\n",
    "    method = df_im.iloc[i]['method']\n",
    "    lamb = df_im.iloc[i]['lambda']\n",
    "    alpha = df_im.iloc[i]['alpha']\n",
    "    fig = px.scatter(vars(df_im['ei_model'].iloc[i].train_history), y='total_loss', color='fair_loss')\n",
    "    fig.update_layout(\n",
    "        title_text = f'Synthetic Dataset | {method} | alpha: {alpha} | lambda: {lamb}',\n",
    "        # template='plotly_dark',\n",
    "        **default_layout(width=1000, height=450), \n",
    "                      )\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(df_im[(df_im['alpha']>0)].sort_values(['method', 'lambda']), x='lambda', y='fair_loss', hover_data='lambda', color='rei_disparity', facet_col='method', color_continuous_scale=['lightblue', 'blue', 'purple', 'orange', 'red'])\n",
    "\n",
    "fig.update_layout(\n",
    "    title_text = f'{dataset.name.capitalize()} Dataset | Gradient Descent | alpha: {params[\"alpha\"][0]}', \n",
    "    **default_layout(np.inf))\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(df_im.sort_values('fair_loss'), x='fair_loss', y='error', color='method')\n",
    "fig.update_layout(\n",
    "    **default_layout()\n",
    "    )\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
