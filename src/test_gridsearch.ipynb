{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from model import LR\n",
    "from data import FairnessDataset, SyntheticDataset\n",
    "from ei_effort import Optimal_Effort\n",
    "from ei_utils import *\n",
    "from ei_model_dev import FairBatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SyntheticDataset(seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.2218,  0.5095, -0.1942,  0.3345])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tensors, val_tensors, test_tensors = dataset.tensor(z_blind=False)\n",
    "train_dataset = FairnessDataset(*train_tensors, dataset.imp_feats)\n",
    "val_dataset = FairnessDataset(*val_tensors, dataset.imp_feats)\n",
    "test_dataset = FairnessDataset(*test_tensors, dataset.imp_feats)\n",
    "\n",
    "model_params = torch.load('../ei_model.pkl')\n",
    "model = LR(train_dataset.X.shape[1])\n",
    "model.load_state_dict(model_params)\n",
    "\n",
    "for module in model.layers:\n",
    "    if hasattr(module, 'weight'):\n",
    "        weights_0 = module.weight.data\n",
    "    if hasattr(module, 'bias'):\n",
    "        bias_0 = module.bias.data\n",
    "\n",
    "theta_0 = torch.cat((weights_0[0], bias_0), 0)\n",
    "theta_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.1\n",
    "tau = 0.5\n",
    "thetas = torch.from_numpy(generate_grid(theta_0.numpy(), alpha, n=15, ord=np.inf)).float()\n",
    "effort_model = Optimal_Effort(tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 14680/38416 [00:04<00:06, 3561.04it/s]"
     ]
    }
   ],
   "source": [
    "Y_hat = model(test_dataset.X).reshape(-1).detach().numpy()\n",
    "X = test_dataset.X[(Y_hat<tau),:]\n",
    "Z = test_dataset.Z[(Y_hat<tau)]\n",
    "X_hat_max = effort_model(model, test_dataset, X)\n",
    "\n",
    "losses = []\n",
    "loss_fn = torch.nn.BCELoss(reduction = 'mean')\n",
    "\n",
    "for theta in tqdm(thetas):\n",
    "    weights, bias = theta[:-1].clone().reshape(1, -1), theta[[-1]].clone()\n",
    "\n",
    "    model_adv = deepcopy(model)\n",
    "    for module in model_adv.layers:\n",
    "        if hasattr(module, 'weight'):\n",
    "            module.weight.data = weights.float()\n",
    "        if hasattr(module, 'bias'):\n",
    "            module.bias.data = bias.float()\n",
    "            \n",
    "    Y_hat_max = model_adv(X_hat_max).reshape(-1)\n",
    "    \n",
    "    fair_loss = 0.\n",
    "    loss_mean = loss_fn(Y_hat_max, torch.ones(len(Y_hat_max)))\n",
    "    loss_z = torch.zeros(len(dataset.sensitive_attrs))\n",
    "    for z in test_dataset.sensitive_attrs:\n",
    "        z = int(z)\n",
    "        group_idx = (Z == z)\n",
    "        if group_idx.sum() == 0:\n",
    "            continue\n",
    "        loss_z[z] = loss_fn(Y_hat_max[group_idx], torch.ones(group_idx.sum()))\n",
    "        fair_loss += torch.abs(loss_z[z] - loss_mean)\n",
    "    \n",
    "    losses.append(fair_loss.detach().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_i = np.argmax(losses)\n",
    "fair_loss = losses[max_i]\n",
    "theta_r = thetas[max_i]\n",
    "weights, bias = theta_r[:-1].clone().reshape(1, -1), theta_r[[-1]].clone()\n",
    "\n",
    "model_adv = deepcopy(model)\n",
    "for module in model_adv.layers:\n",
    "    if hasattr(module, 'weight'):\n",
    "        module.weight.data = weights.float()\n",
    "    if hasattr(module, 'bias'):\n",
    "        module.bias.data = bias.float()\n",
    "        \n",
    "X_hat_max = effort_model(model_adv, test_dataset, test_dataset.X)\n",
    "Y_hat_max = model_adv(X_hat_max).reshape(-1).detach().numpy()\n",
    "\n",
    "accuracy, ei_disparity = model_performance(test_dataset.Y.detach().numpy(), test_dataset.Z.detach().numpy(), Y_hat, Y_hat_max, tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha               |   0.1\n",
      "Accuracy            |   0.608\n",
      "Fairness Loss       |   0.079\n",
      "EI Disparity        |   0.103\n",
      "theta_0             |   tensor([ 0.2218,  0.5095, -0.1942,  0.3345])\n",
      "theta_r             |   tensor([ 0.3075,  0.5952, -0.1085,  0.2345])\n",
      "alphas              |   tensor([0.0857, 0.0857, 0.0857, 0.1000])\n"
     ]
    }
   ],
   "source": [
    "alphas = (theta_r-theta_0).abs()\n",
    "print(f'alpha               |   {alpha}')\n",
    "print(f'Accuracy            |   {accuracy:.3f}')\n",
    "print(f'Fairness Loss       |   {fair_loss:.3f}')\n",
    "print(f'EI Disparity        |   {ei_disparity:.3f}')\n",
    "print(f'theta_0             |   {theta_0}')\n",
    "print(f'theta_r             |   {theta_r}')\n",
    "print(f'alphas              |   {np.round(alphas,4)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ei_model = FairBatch(model, effort_model, 1e-7)\n",
    "pga_Y_hat, pga_Y_hat_max, pga_fair_loss = ei_model.predict_r(test_dataset, alpha)\n",
    "\n",
    "pga_accuracy, pga_ei_disparity = model_performance(test_dataset.Y.detach().numpy(), test_dataset.Z.detach().numpy(), pga_Y_hat, pga_Y_hat_max, tau)\n",
    "\n",
    "for module in ei_model.model_adv.layers:\n",
    "    if hasattr(module, 'weight'):\n",
    "        pga_weights_r = module.weight.data\n",
    "    if hasattr(module, 'bias'):\n",
    "        pga_bias_r = module.bias.data\n",
    "pga_theta_r = torch.cat((pga_weights_r[0], pga_bias_r), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PGA Results\n",
      "alpha               |   0.1\n",
      "Accuracy            |   0.608\n",
      "Fairness Loss       |   0.089\n",
      "EI Disparity        |   0.113\n",
      "theta_0             |   tensor([ 0.2218,  0.5095, -0.1942,  0.3345])\n",
      "theta_r             |   tensor([ 0.3218,  0.6095, -0.0942,  0.2345])\n",
      "alphas              |   tensor([0.1000, 0.1000, 0.1000, 0.1000])\n"
     ]
    }
   ],
   "source": [
    "alphas = (pga_theta_r-theta_0).abs()\n",
    "print('PGA Results')\n",
    "print(f'alpha               |   {alpha}')\n",
    "print(f'Accuracy            |   {pga_accuracy:.3f}')\n",
    "print(f'Fairness Loss       |   {pga_fair_loss:.3f}')\n",
    "print(f'EI Disparity        |   {pga_ei_disparity:.3f}')\n",
    "print(f'theta_0             |   {theta_0}')\n",
    "print(f'theta_r             |   {pga_theta_r}')\n",
    "print(f'alphas              |   {np.round(alphas,4)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
